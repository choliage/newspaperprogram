# **爬蟲專案**  
### **待辦**  
~~額外建立全自動掃關鍵字的另外一部分~~  
匯出資料雲端化  
~~EPU自動抓取~~
~~EPU指數自動建立~~
~~輸出路徑自動化(可選)~~   
~~Excel 計畫以周圍單位劃分~~  
~~一周內新聞/標題/日期/網址~~  

### 現功能  
### **自動排程掃描**  
>一小時一次，聯合、中時已通過一晚上的測試以及凌晨長跑  
>聯合:不會一次丟一堆，所以只保持最簡單的檢索是否有新文章，缺點是要是一個colume更新一堆，會有漏掉的問題，但按照觀察，並不算太需要關注這個問題，一小時一次的頻率算夠用了。  
>中時:更新文章會在5:10丟一堆，因此一次暴力解掃八頁。已確認可行。  
>自由:暫時通過了每小時檢驗，如果有需要可以回推到很久以前，已攻破li page=n的限制，一次掃描一個colume約產生83篇文章的掃描量。另外，我有特別換個寫法讓他偵測網頁的各類別新聞，並不是單純抓已有的，而是未來的也會一起抓。  
### **匯出各周excel**  
>直接用time套件解決要去想日期問題的部分，Excel的匯出也是直接交給套件處理，狀況很好，依照這是今年的第幾周去分類。  
### **檢查是否已有檔案**  
>最大程度避免不必要的資源浪費，而且也節省了動爬網站需要開網站的這個動作，基本上已有的篩選機制可以忽略url跟檔名已存在，標準化url也解決了網域跳轉以及https://www.沒有在動態爬蟲中被開的問題。每小時掃一次的整個過程應不超過15分鐘(每篇15秒為計算，中時跟自由可能需要較多時間)。  
### **Newspaper轉txt**  
>我自己最喜歡的部分，這套件直接解決了過去OCR之類的可能掃到多餘文字的問題，準確率在排除url問題後已經提升，個人認為約有98%左右(2%的問題出在抓到了如延伸閱讀的部分)。  
### **輸出全自動化以及高度模組化的main檔案**  
>現在已經可以完全自動爬蟲，漏抓的問題已得有解決，高度模組化的檔案也可以以更高的效率進行補綴，僅需搞清楚param_name的名稱即可，同一網域需要爬蟲則沿用已有的{}就可以進行多的補充。 
### **轉出為.exe檔案並且完成全自動抓cd**
>原本是使用絕對路徑進行coding，但現在改用os套件進行路徑的獲得，現在不再需要在使用前先改路徑才能使用，也不需要再額外弄一堆有的沒有，直接一鍵爬蟲，太厲害了波風水門。
### **GUI和Log系統的建立**
>GUI建立，擺脫了cmd，畫面會變得更加視覺化跟更現代化，並且Log系統的建立有助於未來我找有沒有哪裡長期跑下來有問題。  
### **Excel EPU自動檢索與各日文章結合**
>目前已經可以讓爬蟲下來的文章進行下一步的epu檢索以及按照報社分類，並匯出圖表。

## **使用說明**  
點開https://github.com/choliage/newspaperprogram/tree/main/dist  
下載最新版本的P.O.E. System  
安裝後運行即可  
GUI按鈕可隨時按下檢查  


