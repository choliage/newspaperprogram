import os
import time
from datetime import datetime
from scraper_udn import fetch_udn_articles_and_save
from scraper_ct import fetch_ct_articles_and_save
from datetime import datetime, timedelta
from scraper_ltn import fetch_ltn_articles_and_save
import pandas as pd
import re
import random

BASE_DIR = r"C:\Users\lolee\Desktop\studios\çˆ¬èŸ²\æ•´åˆçµæžœ"

TARGETS = [
    {
        'label': 'è¯åˆåœ‹éš›',
        'url': 'https://udn.com/news/cate/2/7225',
        'fetch_and_save': fetch_udn_articles_and_save,
        'param_name': 'page_url'
    },
    {
        'label': 'è¯åˆç”¢ç¶“',
        'url': 'https://udn.com/news/cate/2/6644',
        'fetch_and_save': fetch_udn_articles_and_save,
        'param_name': 'page_url'
    },
{
        'label': 'ä¸­æ™‚',
        'url': 'https://www.chinatimes.com/money/total?page=1&chdtv',
        'fetch_and_save': fetch_ct_articles_and_save,
        'param_name': 'index_url'
    },
    {
        'label': 'ä¸­æ™‚',
        'url': 'https://www.chinatimes.com/money/total?page=2&chdtv',
        'fetch_and_save': fetch_ct_articles_and_save,
        'param_name': 'index_url'
    },
    {
        'label': 'ä¸­æ™‚',
        'url': 'https://www.chinatimes.com/money/total?page=3&chdtv',
        'fetch_and_save': fetch_ct_articles_and_save,
        'param_name': 'index_url'
    },
    {
        'label': 'ä¸­æ™‚',
        'url': 'https://www.chinatimes.com/money/total?page=4&chdtv',
        'fetch_and_save': fetch_ct_articles_and_save,
        'param_name': 'index_url'
    },
    {
    'label': 'è‡ªç”±æ™‚å ±',
    'url': 'https://ec.ltn.com.tw/',
    'fetch_and_save': fetch_ltn_articles_and_save,
    'param_name': 'index_url'
    }


]
    

def scan_once():
    print(f"\nâ° {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - é–‹å§‹æ“·å–")
    print(f"\nç¨‹åº ðŸ’€ABSOLUTE-CINEMAðŸ’€ å·²ç¶“å•Ÿå‹•")
    for target in TARGETS:
        print(f"ðŸ“¡ æ“·å–ï¼š{target['label']}")
        try:
            kwargs = {
                target['param_name']: target['url'],
                'source_label': target['label'],
                'output_dir': BASE_DIR
            }
            target['fetch_and_save'](**kwargs)
        except Exception as e:
            print(f"âŒ æ“·å–å¤±æ•—ï¼š{target['label']}ï¼ŒåŽŸå› ï¼š{e}")
    print("âœ… æ“·å–å®Œæˆ\n")
    # ðŸ”½ è‡ªå‹•ç”¢å‡º Excelï¼šæ¯é€±ç‚ºå–®ä½å½™æ•´æ–°èžæ¨™é¡Œã€æ—¥æœŸã€ç¶²å€
    def postprocess_to_excel(output_root_dir):
        
        from datetime import datetime
        from collections import defaultdict

        def extract_info_from_txt(filepath):
            with open(filepath, "r", encoding="utf-8") as f:
                content = f.read()
            title_match = re.search(r"æ¨™é¡Œ:\s*(.*)", content)
            url_match = re.search(r"é€£çµ:\s*(https?://[^\s]+)", content)
            date_match = re.search(r"(\d{4}-\d{2}-\d{2})[_ ]?\d{0,4}", os.path.basename(filepath))
            return {
                "date": date_match.group(1) if date_match else None,
                "title": title_match.group(1).strip() if title_match else "ç„¡æ¨™é¡Œ",
                "url": url_match.group(1).strip() if url_match else "ç„¡é€£çµ"
            }

        def group_by_week(data):
            grouped = defaultdict(list)
            for item in data:
                try:
                    date_obj = datetime.strptime(item["date"], "%Y-%m-%d")
                    year, week, _ = date_obj.isocalendar()
                    key = f"{year}_W{week:02d}"
                    grouped[key].append(item)
                except:
                    continue
            return grouped

        for source in os.listdir(output_root_dir):
            source_path = os.path.join(output_root_dir, source)
            if not os.path.isdir(source_path):
                continue

            all_items = []
            for fname in os.listdir(source_path):
                if fname.endswith(".txt"):
                    info = extract_info_from_txt(os.path.join(source_path, fname))
                    if info["date"]:
                        all_items.append(info)

            by_week = group_by_week(all_items)

            output_dir = os.path.join(source_path, "excel_by_week")
            os.makedirs(output_dir, exist_ok=True)

            for week_key, items in by_week.items():
                df = pd.DataFrame(items)[["date", "title", "url"]]
                df.sort_values("date", inplace=True)
                out_path = os.path.join(output_dir, f"{week_key}.xlsx")
                df.to_excel(out_path, index=False)

            print(f"ðŸ“Š {source} åŒ¯å‡º Excel å®Œæˆï¼ˆå…± {len(by_week)} é€±ï¼‰")

    postprocess_to_excel(BASE_DIR)

if __name__ == "__main__":
    print("ðŸ” æ¯å°æ™‚æ–°èžè‡ªå‹•æ“·å–å•Ÿå‹•ä¸­...")
    while True:
        start_time = datetime.now()
        print(f"\nâ° æœ¬è¼ªé–‹å§‹æ™‚é–“ï¼š{start_time.strftime('%Y-%m-%d %H:%M:%S')}")

        scan_once()

        end_time = datetime.now()
        next_run_time = end_time + timedelta(hours=1)
        sleep_seconds = (next_run_time - datetime.now()).total_seconds()

        print(f"âœ… æœ¬è¼ªå®Œæˆæ™‚é–“ï¼š{end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ðŸ• é è¨ˆä¸‹ä¸€è¼ªé–‹å§‹æ™‚é–“ï¼š{next_run_time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(random.choice([
            "è·‘éƒ½è·‘å®Œäº†ï¼Œå…ˆåŽ»å–å€‹èŒ¶å§",
            "å…„é˜¿ï¼Œå±…ç„¶æˆåŠŸäº†ï¼Œçµ•å°çš„é›»å½±é™¢ðŸ’€",
            "ç”·äººæˆ‘ç½é ­æˆ‘èªª (Man what can I say)",
            "åˆè·‘ä¸€è¼ªï¼Œå‡Œæ™¨å››é»žå« Kobe æŠŠæˆ‘è¼‰èµ°å¥½äº†"
        ]))

        if sleep_seconds > 0:
            time.sleep(sleep_seconds)