import os
import logging
import re
import requests
import time
from datetime import datetime
from urllib.parse import urljoin
from bs4 import BeautifulSoup
from newspaper import Article
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

def load_done_urls(file_path):
    if os.path.exists(file_path):
        with open(file_path, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f)
    return set()

def normalize_url(url):

    parsed = re.sub(r'^https?://(www\.)?', '', url).rstrip('/')
    return parsed.lower()

def append_done_url(file_path, url):
    with open(file_path, "a", encoding="utf-8") as f:
        f.write(url + "\n")

def get_final_url_js(url):
    logging.info(f"ğŸŒ ä½¿ç”¨ Selenium å˜—è©¦è·³è½‰: {url}")
    options = Options()
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")
    options.add_argument("--use-gl=swiftshader")
    driver = webdriver.Chrome(options=options)
    try:
        driver.get(url)
        time.sleep(5)
        return driver.current_url
    except Exception as e:
        logging.warning(f"âš ï¸ Selenium å¤±æ•—: {e}")
        return url
    finally:
        driver.quit()

def get_final_url(original_url):
    logging.info(f"ğŸ”— å˜—è©¦è§£æè·³è½‰ç¶²å€: {original_url}")
    try:
        res = requests.get(original_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(res.text, "lxml")
        meta = soup.find("meta", attrs={"http-equiv": "refresh"})
        if meta and "content" in meta.attrs:
            content = meta["content"]
            if "url=" in content.lower():
                refresh_url = content.split("url=")[-1].strip()
                return urljoin(original_url, refresh_url)
        if "udn.com/news/story" in res.url:
            return get_final_url_js(original_url)
        return res.url
    except Exception as e:
        logging.error(f"âŒ requests è·³è½‰å¤±æ•—: {e}")
        return get_final_url_js(original_url)

def sanitize_filename(title):
    return re.sub(r'[\\/:*?"<>|]', "_", title)[:50]

def fetch_udn_articles_and_save(page_url, output_dir, source_label="è¯åˆ"):
    logging.info(f"ğŸš© é€²å…¥ fetch_udn_articles_and_saveï¼š{page_url}")
    subdir = os.path.join(output_dir, source_label)
    done_file = os.path.join(subdir, "done_urls.txt")
    done_urls = load_done_urls(done_file)
    os.makedirs(subdir, exist_ok=True)
  
    try:
        res = requests.get(page_url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        soup = BeautifulSoup(res.content, 'lxml')
        blocks = soup.find_all('div', class_='story-list__news')
        logging.info(f"ğŸ” æ‰¾åˆ° {len(blocks)} ç¯‡æ–‡ç« å€å¡Š")
        

        for i, story in enumerate(blocks, 1):
            h2 = story.find('h2')
            if not h2:
                continue
            a = h2.find('a')
            if not a or not a.get('href'):
                continue

            title = a.text.strip()
            link = "https://udn.com" + a['href']


            if link in done_urls:
                logging.info(f"   â© é æª¢ï¼šå·²è™•ç†éï¼ˆURL é‡è¤‡ï¼‰ï¼Œå®Œå…¨ç•¥é")
                continue


            final_url = get_final_url(link)

            logging.info(f"  {i:02d}. å˜—è©¦æ“·å–æ–‡ç« ï¼š{title}")
            try:
                article = Article(final_url, language='zh')
                article.download()
                article.parse()
                if len(article.text.strip()) < 30:
                    logging.info("   âš ï¸ å…§å®¹éçŸ­ï¼Œç•¥é")
                    continue

                publish_time = article.publish_date.strftime("%Y-%m-%d_%H%M") if article.publish_date else datetime.now().strftime("%Y-%m-%d_%H%M")
                clean_title = sanitize_filename(article.title or title)
                filename = f"{publish_time}_{source_label}_{clean_title}.txt"
                filepath = os.path.join(subdir, filename)

                with open(filepath, "w", encoding="utf-8") as f:
                    f.write(f"æ¨™é¡Œ: {article.title}\n")
                    f.write(f"é€£çµ: {final_url}\n\n")
                    f.write(article.text)
                append_done_url(done_file, link)   

                logging.info(f"   âœ… å·²å„²å­˜ï¼š{filename}")
            except Exception as e:
                logging.error(f"   âŒ æ“·å–å¤±æ•—: {e}")

    except Exception as outer_e:
        logging.error(f"âŒ ç„¡æ³•è§£æä¸»é é¢ï¼š{outer_e}")
